# Vanila Transformer

---
layout: single
title: "Vanila Transformer"
data: 2023-10-04 17:00:00
---

Transformerì— ê´€í•œ ì˜ ì‘ì„±ëœ ë§ì€ ê¸€ë“¤ì´ ìˆì§€ë§Œ, 
ë³¸ Transformer ì‹œë¦¬ì¦ˆì—ì„œëŠ” ì‰½ê²Œ ì„¤ëª…í•˜ë©´ì„œë„ ì™œ ì´ë ‡ê²Œ ëª¨ë¸ì˜ êµ¬ì¡°ê°€ ì„¤ê³„ë˜ì—ˆëŠ” ì§€ ê·¸ë¦¬ê³  ì½”ë“œê¹Œì§€ ê°™ì´ ì„¤ëª…í•˜ì—¬ ì´í•´í•  ìˆ˜ ìˆëŠ” ê¸€ì´ ë˜ë ¤ê³  í•©ë‹ˆë‹¤.

- Seq2Seqì˜ íë¦„
- Transformer - Overview
- Transformer - Deep Dive

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

class SelfAttention(nn.Module):
    def __init__(self, input_dim: int) -> None:
        """Initialize the SelfAttention module.

        Args:
            input_dim (int): The size of the input dimension.

        The SelfAttention module consists of three linear layers that transform
        the input to create queries, keys, and values. This is a standard practice
        in attention mechanisms to capture different aspects of the input.
        """
        super(SelfAttention, self).__init__()
        self.input_dim = input_dim

        # Linear transformations for creating queries, keys, and values
        self.query_transform = nn.Linear(input_dim, input_dim)
        self.key_transform = nn.Linear(input_dim, input_dim)
        self.value_transform = nn.Linear(input_dim, input_dim)

        # Softmax for normalizing attention scores to probabilities
        self.softmax = nn.Softmax(dim=2)
        
    def forward(self, x: Tensor) -> Tensor:
        """Forward pass of the SelfAttention module.

        Args:
            x (Tensor): The input tensor of shape (batch_size, seq_len, input_dim).

        Returns:
            Tensor: The output tensor after applying self attention.

        The forward pass computes attention scores and applies these to
        the value vectors. The attention scores are scaled down by the square root
        of the input dimension to prevent large values that could push the softmax
        function into regions where it has extremely small gradients (saturation).
        """
        # Create query, key, and value vectors
        queries = self.query_transform(x)
        keys = self.key_transform(x)
        values = self.value_transform(x)

        # Calculate the attention scores with scaling for numerical stability
        scores = torch.bmm(queries, keys.transpose(1, 2)) / (self.input_dim ** 0.5)
        
        # Normalize the attention scores to probabilities
        attention_probs = self.softmax(scores)
        
        # Apply the attention to the values
        weighted_values = torch.bmm(attention_probs, values)
        
        return weighted_values
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MultiHeadAttention(nn.Module):
    """
    Implements a Multi-Head Attention mechanism.
    
    Args:
        input_dim (int): Size of the input dimension.
        num_heads (int): Number of attention heads.
        
    Attributes:
        num_heads (int): The number of separate attention heads.
        head_dim (int): Dimension of each attention head.
        query_layer (nn.Linear): Linear layer for projecting input to query space.
        key_layer (nn.Linear): Linear layer for projecting input to key space.
        value_layer (nn.Linear): Linear layer for projecting input to value space.
        output_layer (nn.Linear): Linear layer to project concatenated outputs.
    """
    
    def __init__(self, input_dim: int, num_heads: int):
        super(MultiHeadAttention, self).__init__()
        assert input_dim % num_heads == 0, "input_dim must be divisible by num_heads"
        
        self.num_heads = num_heads
        self.head_dim = input_dim // num_heads
        
        # Combined linear layers for queries, keys, and values
        self.query_layer = nn.Linear(input_dim, input_dim)
        self.key_layer = nn.Linear(input_dim, input_dim)
        self.value_layer = nn.Linear(input_dim, input_dim)
        
        # Linear layer to combine the outputs of the different heads
        self.output_layer = nn.Linear(input_dim, input_dim)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass for the Multi-Head Attention layer.
        
        Args:
            x (torch.Tensor): A tensor of shape (batch_size, seq_length, input_dim)
                containing the input feature set.
                
        Returns:
            torch.Tensor: The output of the Multi-Head Attention mechanism, 
                a tensor of the same shape as the input x.
        """
        batch_size = x.size(0)
        
        # Apply the linear layers and split into multiple heads
        # Reshape from (batch_size, seq_length, input_dim) to
        # (batch_size, num_heads, seq_length, head_dim)
        queries = self.query_layer(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        keys = self.key_layer(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        values = self.value_layer(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention
        # The scaling factor (1/sqrt(head_dim)) helps in stabilizing gradients during training.
        scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention = F.softmax(scores, dim=-1)
        
        # Combine the attended values for each head
        head_output = torch.matmul(attention, values).transpose(1, 2).contiguous()
        
        # Concatenate the heads' outputs and reshape
        # The final shape is (batch_size, seq_length, input_dim)
        concatenated = head_output.view(batch_size, -1, self.num_heads * self.head_dim)
        
        # Final linear layer to combine heads' outputs
        output = self.output_layer(concatenated)
        return output
```

# 1. Overview ğŸ¤–

![Figure. Transformer êµ¬ì¡°](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled.png)

Figure. Transformer êµ¬ì¡°

**[Attention Is All You Need (Vaswani et al,. 2017)](https://arxiv.org/abs/1706.03762)**ì— ì²˜ìŒ ì†Œê°œëœ ì´í›„ language translation, language modelling, and text classification ë“± ë‹¤ì–‘í•œ ì˜ì—­ì— ì ìš©ë˜ê³  ìˆìœ¼ë©°, ìµœê·¼ ì˜ìƒ ì²˜ë¦¬ ì˜ì—­ì—ì„œë„ ë˜í•œ SOTAëª¨ë¸(ì˜ˆì‹œ: [ViT](https://arxiv.org/abs/2010.11929))ë¡œì¨ ì‚¬ìš©ëœë‹¤. CNNì´ë‚˜ RNN êµ¬ì¡° ì—†ì´, **multi-head self-attention** êµ¬ì¡°ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²Œ ê°€ì¥ í° íŠ¹ì§•ì´ë‹¤. ì´ë¥¼ í†µí•´ì„œ RNNê³¼ ë‹¬ë¦¬ ê¸´ ë¬¸ì¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

> *In this work we propose the **Transformer**, a model architecture eschewing recurrence and instead **relying entirely on an attention mechanism to draw global dependencies between input and output.** The Transformer allows for significantly more **parallelization** and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.

From **[Attention Is All You Need (Vaswani et al,. 2017)](https://arxiv.org/abs/1706.03762)***
> 

## 1.1. Previous Works

![Figure: ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ê¸°ê³„ ë²ˆì—­ ë°œì „ ê³¼ì •](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Screenshot_2023-02-24_at_4.58.13_PM.png)

Figure: ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ê¸°ê³„ ë²ˆì—­ ë°œì „ ê³¼ì •

[Reference: [https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/seq2seq_4.mp4)

Reference: [https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

ì¼ë°˜ì ìœ¼ë¡œ, ê¸°ê³„ ë²ˆì—­ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì€ ìœ„ì™€ ê°™ì´ **encoder-decoder** í˜•íƒœë¥¼ ë”°ë¥´ê²Œ ëœë‹¤. ê¸°ì¡´ì—ëŠ” RNN êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ ê¸°ê³„ ë²ˆì—­ì„ ìœ„í•œ encoder-decoder êµ¬ì¡°ë¥¼ êµ¬í˜„í•˜ì˜€ëŠ”ë°, RNN êµ¬ì¡°ì— ë”°ë¥¸ ì•„ë˜ ë‘ ë¬¸ì œê°€ ì¡´ì¬í•˜ì—¬ ì ì°¨ attention mechanismìœ¼ë¡œ ëŒ€ì²´ëœë‹¤. ì•„ë˜ ê¸°ì¡´ ì—°êµ¬ë“¤ì„ í†µí•´ì„œ ê·¸ ë¬¸ì œì ë“¤ê³¼ ë°œì „ ê³¼ì •ì„ í™•ì¸í•´ë³¸ë‹¤. 

## 1.1.1 Sequence to Sequence Learning with Neural Networks (NIPS 2014)

![Screenshot 2023-02-24 at 4.55.16 PM.png](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Screenshot_2023-02-24_at_4.55.16_PM.png)

- **Seq2Seq:** Multi-layered LSTM ê¸°ë°˜ì˜ encoderâ†’**fixed-length context vector**â†’decoder ê¸°ê³„ ë²ˆì—­ ì•„í‚¤í…ì²˜
- **ë³‘ë ¬í™”(parallelization)**ì— ì œí•œì 
    - Critical at **longer sequence lengths**, as **memory constraints** limit batching across examples
    - High latency, low throughput
- **Fixed-length context vector**: **ê³ ì •ëœ í¬ê¸°**ì˜ context vector ì— ë”°ë¥¸ **ë³‘ëª©(bottleneck)** í˜„ìƒ
    - í•˜ë‚˜ì˜ **ê³ ì •ëœ í¬ê¸°ì˜** ë¬¸ë§¥ ë²¡í„°ê°€ ì†ŒìŠ¤ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì••ì¶•í•´ì„œ ê°€ì§€ê³  ìˆì–´ì•¼ í•˜ë¯€ë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì œí•œí•˜ëŠ” bottleneckìœ¼ë¡œ ì‘ìš©í•˜ì—¬ long-term dependencyê°€ ë–¨ì–´ì§. â†’ ì¶”í›„ attention networkì˜ ë“±ì¥í•˜ê²Œ ëœ ë°°ê²½

ì¦‰, ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ **ì†ŒìŠ¤ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ + í˜„ì¬ ì¸ì½”ë”© ë‹¨ê³„ê¹Œì§€ ì¶œë ¥ëœ ì •ë³´**ë¥¼ ì…ë ¥ê°’ìœ¼ë¡œ ë„£ê³ ì í•˜ëŠ” ê²ƒì´ë‹¤.  **

## 1.1.2 Neural Machine Translation by Jointly Learning to Align and Translate (ICLR 2015)

![Encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/seq2seq.gif)

Encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation.

<aside>
ğŸ’¡ **Attention mechanism** became an integral part of sequence modeling and transduction models in various tasks.

</aside>

Seq2Seq ëª¨ë¸ì—ì„œ **fixed-length context vector**ë¡œ ****ëª¨ë“  í•„ìš”í•œ ì •ë³´ë¥¼ ì••ì¶•í•˜ê²Œ ë˜ë©´ì„œ ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ”ë‹¤. Seq2Seq + Attention ì—ì„œëŠ” ë””ì½”ë”ê°€ ì¸ì½”ë”ì˜ ëª¨ë“  ìŠ¤í…ì˜ hidden statesë¥¼ë°›ì•„ Attention êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ decoding ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.

- Encoder: a bidirectional RNN
- Decoder: proposed **attention model**
    - Attention mechanism: **a weighted sum of the input hidden states**

![Figure. Attention](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/1_wBHsGZ-BdmTKS7b-BtkqFQ.gif)

Figure. Attention

Encoderì˜ ëª¨ë“  ìŠ¤í…ì˜ hidden statesë¥¼ decoderì˜ hidden stateì™€ scoreë¥¼ ê³„ì‚°í•˜ì—¬ ë¬¸ì¥ì—ì„œ ì§‘ì¤‘í•  ë¶€ë¶„ì„ ì„ íƒì ìœ¼ë¡œ ë³´ê³  predictionì„ í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

âš ï¸Â **Limitation**:Â RNNì— attention mechanismì„ ì´ìš©í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” RNNìœ¼ë¡œ ì¸í•´ (1) ëŠë¦° í•™ìŠµ ë° ì¶”ë¡  ì†ë„ì™€ (2) ì¶©ë¶„í•˜ì§€ ì•Šì€ ì„±ëŠ¥ì´ ì œí•œì ì¸ ì—°êµ¬ì˜€ë‹¤. íŠ¹íˆ, ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê°–ê³  ìˆë‹¤.

## 1.2. Model Architecture

![Figure. The full model architecture of the transformer. (Image source: Fig 1 & 2 inÂ [Vaswani, et al., 2017](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%201.png)

Figure. The full model architecture of the transformer. (Image source: Fig 1 & 2 inÂ [Vaswani, et al., 2017](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).)

### Input Embedding

- Input Embedding in Code
    
    ```python
    class Embeddings(nn.Module):
    	"""Convert the input tokens to vectors of dimension d_model."""
    	def __init__(self, d_model, vocab):
            super(Embeddings, self).__init__()
            self.embedding = nn.Embedding(vocab, d_model)
            self.d_model = d_model
    
    	def forward(self, x):
    		return self.embedding(x) * math.sqrt(self.d_model)
    ```
    

![[https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding#c8143ffb-4ce2-4173-8f7d-501d3df723cc](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding#c8143ffb-4ce2-4173-8f7d-501d3df723cc)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%202.png)

[https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding#c8143ffb-4ce2-4173-8f7d-501d3df723cc](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding#c8143ffb-4ce2-4173-8f7d-501d3df723cc)

Word Embedding: ê° ë‹¨ì–´ë¥¼ indexì— ë§¤ì¹­í•˜ê³  embedding layerë¥¼ ê±°ì³ $d_{model} = 512$ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ pre-trained embedding layerë¥¼ í™œìš©í•œë‹¤.

![Word Embeddingì˜ ì˜ˆì‹œ. ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ë‹¨ì–´ë¼ë¦¬ embedding spaceì—ì„œ clusterë¥¼ ì´ë£¨ê²Œ ëœë‹¤. [https://www.ruder.io/word-embeddings-1/](https://www.ruder.io/word-embeddings-1/)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%203.png)

Word Embeddingì˜ ì˜ˆì‹œ. ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ë‹¨ì–´ë¼ë¦¬ embedding spaceì—ì„œ clusterë¥¼ ì´ë£¨ê²Œ ëœë‹¤. [https://www.ruder.io/word-embeddings-1/](https://www.ruder.io/word-embeddings-1/)

### Positional Embedding

![[https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/positional_encoding.gif)

[https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding](https://www.blossominkyung.com/deeplearning/transfomer-positional-encoding)

TransformerëŠ” RNNê³¼ ë‹¬ë¦¬ ì…ë ¥ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ëŒ€ì‹  ë³‘ë ¬ë¡œ í•œë²ˆì— ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì—,  ê° ë‹¨ì–´ì˜ ìˆœì„œë¥¼ Transformerì— ì „ë‹¬í•  ìˆ˜ë‹¨ì´ í•„ìš”í•˜ë‹¤. Positional embeddingì€ input embedding vectorì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë‹´ì€ ë²¡í„°$(\vec{p_t})$ë¥¼ ë”í•´ì¤€ë‹¤. 

![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%204.png)

![Reference: [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%205.png)

Reference: [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

- ë¶€ë¡: Positional Embedding vs. Learned Positional Embedding
    
    TransformerëŠ” sinusoid-wave ê¸°ë°˜ì˜ positional encodingì„ í™œìš©í•œë‹¤. ì´ëŠ” positional encoding layerë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ë¹„êµí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•˜ë‹¤.
    
    1. positional encoding layer ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ì„±ëŠ¥ ìƒì˜ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ì—ˆë‹¤.
    2. sinusoid-wave ê¸°ë°˜ì˜ ë°©ë²•ì€ í•™ìŠµí•  ë•Œ ë³´ì§€ ëª»í–ˆë˜ ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•˜ëŠ” ë° ë” ìœ ë¦¬í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤.
    
    ì°¸ê³ . [What DoPosition Embeddings Learn? AnEmpirical Study of Pre-Trained Language Model Positional Encoding](https://arxiv.org/abs/2010.04903)
    
- Positional Embedding in Code
    
    ```python
    class PositionalEncoding(nn.Module):
        def __init__(self, dim_model, dropout_p, max_len):
            super().__init__()
            # ë“œë¡­ ì•„ì›ƒ
            self.dropout = nn.Dropout(dropout_p)
    
            # Encoding - From formula
            pos_encoding = torch.zeros(max_len, dim_model)
            positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5
            division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)
    
            pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)
            pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)
    
            # Saving buffer (same as parameter without gradients needed)
            pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)
            self.register_buffer("pos_encoding", pos_encoding)
    
        def forward(self, token_embedding: torch.tensor) -> torch.tensor:
            # Residual connection + pos encoding
            return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])
    ```
    

### TransformerEncoder

![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%206.png)

N = 6ì˜ ë™ì¼í•œ layerë¡œ êµ¬ì„±ë˜ë©°, ê° layerëŠ” 2ê°œì˜ sub-layer (multi-head self-attention and position-wise feedforward networks)ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ê° ë‘ ê°€ì§€ sub-layerëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- **Sub-layer (1): multi-head self-attention mechanism**
    
    ![[https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/attention.gif)
    
    [https://github.com/jessevig/bertviz](https://github.com/jessevig/bertviz)
    
    Attention ë°©ì‹ì„ í†µí•´ ë¬¸ì¥ ë‚´ì—ì„œ ì–´ë–¤ ë¶€ë¶„ì— ì–¼ë§Œí¼ attentionì„ ê°€ì ¸ì•¼ í•˜ëŠ” ì§€ í•™ìŠµí•˜ê²Œ ëœë‹¤. 
    
    ![Screenshot 2023-02-24 at 3.13.43 PM.png](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Screenshot_2023-02-24_at_3.13.43_PM.png)
    
    $$
    Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V
    $$
    
    > Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.
    > 
    
    ### ì™œ â€œMulti-headâ€ ì¼ê¹Œ?
    
    ![multi-head.gif](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/multi-head.gif)
    
    ![As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%207.png)
    
    As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".
    
    ë¬¸ì¥ì€ ëª¨í˜¸í•œ ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì—, ë‹¤ì–‘í•œ ê´€ì (multi-head)ì—ì„œ ë¬¸ì¥ì„ ì´í•´í•´ì•¼í•œë‹¤.. ìœ„ ê·¸ë¦¼ì—ì„œë„, ì²« ë²ˆì§¸ self-attentionì€ `it`ê³¼ `animal` ì˜ attention ê°’ì´ ë†’ì•˜ì§€ë§Œ, ë‘ ë²ˆì§¸ self-attentionì€ `it`ê³¼ `tired`ê°€ attentionì´ ë†’ì•˜ë‹¤.
    
    - Self-attention illustrations
        
        ![[https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/1__92bnsMJy8Bl539G4v93yg.gif)
        
        [https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a)
        
    - **Muti-head attention details**
        
        ![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%208.png)
        
        $$
        Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V
        $$
        
- **Sub-layer (2) position-wise fully connected feed-forward network**
    
    $$
    FFN(x)=max(0, xW_1+b_1)W_2+b_2
    $$
    
    ![Pointwise-ff-nn.gif](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Pointwise-ff-nn.gif)
    
    - Code
        
        ```python
        class PointwiseFeedForward(nn.Module):
            def __init__(self,d_model, d_linear,dropout = 0.2):
                super(PointwiseFeedForward,self).__init__()
                self.W1 = nn.Linear(d_model,d_linear)
                self.W2 = nn.Linear(d_linear, d_model)
                self.dropout = nn.Dropout(dropout)
                
            def forward(x):
                relu = F.relu(self.W1(x))
                output = self.dropout(self.W2(x))
                return output
        ```
        

ë˜í•œ, ê° sub-layerì€ ë’¤ì´ì–´ Add & Norm (=LayerNorm(x + Sublayer(x)))ì„ ë”°ë¥¸ë‹¤. 

- Details on LayerNormalization
    
    For $X=[x_1,x_2,...,x_m]$, LayerNorm normalizes each $x_i$ across all its features such that each sample $x_i$ has 0 mean and unit variance.
    
    ```python
    
    ```
    

### TransformerDecoder

![transformer_decoding_1.gif](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/transformer_decoding_1.gif)

![transformer_decoding_2.gif](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/transformer_decoding_2.gif)

![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%209.png)

N = 6ì˜ ë™ì¼í•œ layerë¡œ êµ¬ì„±ë˜ë©°, ê° layerëŠ” 3ê°œì˜ sub-layer (`multi-head self-attention`, `multi-head cross-attention`, and `position-wise feedforward networks`)ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ê° ì„¸ ê°€ì§€ sub-layerëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%2010.png)

- **Sub-layer (1): masked multi-head self-attention**
    
    ![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%2011.png)
    
    ![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%2012.png)
    
    TransformerëŠ” ì „ì²´ ì…ë ¥ê°’ì„ ì „ë‹¬ë°›ê¸° ë•Œë¬¸ì— ê³¼ê±° ì‹œì ì˜ ì…ë ¥ê°’ì„ ì˜ˆì¸¡í•  ë•Œ ë¯¸ë˜ ì‹œì ì˜ ì…ë ¥ê°’ê¹Œì§€ ì°¸ê³ í•  ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ `Look-ahead Mask` ê¸°ë²•ì„ ì´ìš©í•œ Masked Attention ì„ í™œìš©í•©ë‹ˆë‹¤.
    
    DecoderëŠ” â€œ**Masked**â€ multi-head attentionì„ í™œìš©í•´ i ë²ˆì§¸ í† í°ì€ ië²ˆì§¸ ì´í›„ ê°’ì—ëŠ” independentí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, attention score í–‰ë ¬ì˜ ëŒ€ê°ì„  ìœ—ë¶€ë¶„ì„ `-inf` ë¡œ ë‘ê³  (Look-ahead Mask) Softmaxë¥¼ ì·¨í•˜ì—¬ í•´ë‹¹ ìš”ì†Œë“¤ì˜ Attention Weightë“¤ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ Attention Valueë¥¼ ê³„ì‚°í•  ë•Œ ë¯¸ë˜ ì‹œì ì˜ ê°’ì„ ê³ ë ¤í•˜ì§€ ì•Šë„ë¡ ë§Œë“¤ì–´ì¤€ë‹¤.
    
- **Sub-layer (2): multi-head cross-attention (Encoder-Decoder attention)**
    - **`self`**-attentionì´ ì•„ë‹Œ **`cross`**-attention ì„ ì‚¬ìš©í•œë‹¤.
    - ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ ê°’ì´ ëª¨ë“  ë””ì½”ë”ì— ì…ë ¥ ê°’ ì¤‘ í•˜ë‚˜ë¡œ ì‚¬ìš©.
        
        
- **Sub-layer (3) position-wise fully connected feed-forward network**

## 1.3. Pros and Cons

<aside>
<img src="https://super.so/icon/dark/align-left.svg" alt="https://super.so/icon/dark/align-left.svg" width="40px" /> `Summary` TransformerëŠ” long-sequence modeling ì— ìœ ë¦¬í•˜ë‚˜, computational cost, memory requirement, ê·¸ë¦¬ê³  ë§ì€ í•™ìŠµ ë°ì´í„° ìš”êµ¬ì‚¬í•­ì„ ê³ ë ¤í•œ ëª¨ë¸ë§ì´ í•„ìš”í•¨.

</aside>

<aside>
ğŸ‘ğŸ» Pros

1. Long-sequence modeling
2. ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¹ ë¥¸ í•™ìŠµ
3. ì ì€ inductive biasë¡œ ì¸í•œ ì¼ë°˜í™”ëœ í•™ìŠµì— ìœ ë¦¬
</aside>

<aside>
ğŸ‘ğŸ» Â Cons

1. Computational cost
2. Memory-intensive
3. ë§ì€ í•™ìŠµ ë°ì´í„° ìš”êµ¬
4. Auto-regressive ì˜ˆì¸¡ìœ¼ë¡œ ì¸í•œ ì¶”ë¡  ì‹œ ì˜¤ë˜ ê±¸ë¦°ë‹¤.
</aside>

## 1.4. Transformer ì—°êµ¬ ë™í–¥

1. Model Efficiency
    - Self-Attention ëª¨ë“ˆì€ ê¸´ ì…ë ¥ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ë†’ì€ ì—°ì‚°ê³¼ ë©”ëª¨ë¦¬ ë¹„ìš©ì„ ìš”êµ¬
    - Sparse Attention ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ëŠ” ë°©í–¥: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)
2. Model Generalization
    - Inductive biasê°€ ì ì§€ë§Œ ë§ì€ í•™ìŠµ ë°ì´í„°ê°€ ìˆì–´ì•¼í•¨.
    - Self-supervised learningì„ í†µí•œ ì‚¬ì „ í•™ìŠµì„ í†µí•´ ì¼ë°˜í™” ì„±ëŠ¥ ê·¹ëŒ€í™”.
3. Model Adaptation
    - Specificí•œ downstream taskì— ì ìš©í•˜ê¸° ìœ„í•œ ì—°êµ¬

---

## 2.2. **Training Strategy**

### 2.2.1 Pre-trained Transformer

![Untitled](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%2013.png)

<aside>
ğŸ’¡ â€œRecent work has demonstrated substantial gains on many NLP tasks and benchmarks by *pre-training on a large corpus of text followed by fine-tuning on a specific task*.â€
ì°¸ì¡°: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

</aside>

ìµœê·¼ BERT, GPT ê°™ì€ ëª¨ë¸ì´ ì£¼ëª©ì„ ë°›ê²Œ ëœ ì´ìœ ëŠ” ì„±ëŠ¥ ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ë“¤ì„ ì‚¬ìš©í•˜ë©´ ë¬¸ì„œ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹ ë“± ì–´ë–¤ íƒœìŠ¤í¬ë“ ì§€ ì ìˆ˜ê°€ ì´ì „ ëŒ€ë¹„ í° í­ìœ¼ë¡œ ì˜¤ë¥´ê¸° ë•Œë¬¸ì¸ë°ìš”. BERT, GPT ë”°ìœ„ì˜ ë¶€ë¥˜ëŠ”Â **ë¯¸ë¦¬ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸(pretrained language model)**ì´ë¼ëŠ” ê³µí†µì ì´ ìˆìŠµë‹ˆë‹¤. [https://jalammar.github.io/illustrated-bert/](https://jalammar.github.io/illustrated-bert/)

ì¼ë°˜ì ìœ¼ë¡œ TransformerëŠ” ë§ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ìš”êµ¬í•˜ë©°, ì´ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ê°€ìš© ë°ì´í„°ê°€ ì ì€ HMG-EV í”„ë¡œì íŠ¸ì—ëŠ” Pre-trained Transformerë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ pre-training ë°©ì‹ì€ skip-gram, masked model ë“±ì´ ìˆìŠµë‹ˆë‹¤.

![Masked model ì˜ˆì‹œ](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%2014.png)

Masked model ì˜ˆì‹œ

**ë§ˆìŠ¤í¬ ëª¨ë¸(Masked  Model)**ì€ í•™ìŠµ ëŒ€ìƒ ë¬¸ì¥ì— ë¹ˆì¹¸ì„ ë§Œë“¤ì–´ ë†“ê³  í•´ë‹¹ ë¹ˆì¹¸ì— ì˜¬ ë‹¨ì–´ë¡œ ì ì ˆí•œ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. [BERT: Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805)ê°€ ë§ˆìŠ¤í¬ ì–¸ì–´ëª¨ë¸ë¡œ pre-trainë˜ëŠ” ëŒ€í‘œì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤. ****

![Skip-gram model ì˜ˆì‹œ](Vanila%20Transformer%203844c831810e40a1b93ed48573f04fb7/Untitled%2015.png)

Skip-gram model ì˜ˆì‹œ

**ìŠ¤í‚µ-ê·¸ë¨ ëª¨ë¸(Skip-Gram Model)**ì€ ì–´ë–¤ ë‹¨ì–´ ì•ë’¤ì— íŠ¹ì • ë²”ìœ„ë¥¼ ì •í•´ ë‘ê³  ì´ ë²”ìœ„ ë‚´ì— ì–´ë–¤ ë‹¨ì–´ë“¤ì´ ì˜¬ì§€ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµí•©ë‹ˆë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì€ ì»¨í…ìŠ¤íŠ¸ë¡œ ì„¤ì •í•œ ë‹¨ì–´(íŒŒë€ìƒ‰ ë„¤ëª¨ì¹¸) ì•ë’¤ë¡œ ë‘ ê°œì”© ë³´ëŠ” ìƒí™©ì„ ë‚˜íƒ€ë‚¸ ì˜ˆì‹œì…ë‹ˆë‹¤. Skim-Gram ë°©ì‹ì„ í™œìš©í•œ ëŒ€í‘œì ì¸ ì—°êµ¬ë¡œëŠ” [Word2Vec](https://arxiv.org/abs/1301.3781) ì´ ìˆìŠµë‹ˆë‹¤.****

ìœ ì‚¬í•˜ê²Œ Time-series ë°ì´í„°ì— masked model, skip-gram model ë°©ì‹ì„ ì ìš©í•œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---

- Pre-training large transformers on massive amounts of data has been shown to improve their performance on downstream tasks, reducing the need for task-specific data.
- TransformerëŠ” ë§ì€ computational costì™€ memoryë¥¼ ìš”êµ¬í•  ìˆ˜ ìˆì–´, HMG-EV ì—ì„œ ê°€ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì˜ computation ìˆ˜ì¤€ê³¼ memory í¬ê¸°ì— ë§ì¶° ëª¨ë¸ ê°œë°œì„ í•„ìš”ë¡œ í•  ìˆ˜ ìˆë‹¤.
- Self-attentionì€ sequence lengthì— ëŒ€í•˜ì—¬ $O(n^2)$ì˜ computational complexityë¥¼ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì—, ì ì • ìˆ˜ì¤€ì˜ sequence lengthë¥¼ ê²°ì •í•´ì•¼í•œë‹¤. (ì¦‰, ë„ˆë¬´ ê¸´ ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ê²ƒì€ computational costë¡œ ì¸í•´ ì–´ë ¤ìš¸ ìˆ˜ ìˆë‹¤.)
- TransformerëŠ” ì›ë˜ NLPë¥¼ ìœ„í•´ ê°œë°œë˜ì—ˆê³  SOTA ëª¨ë¸ë¡œ ì‚¬ìš©ë˜ê³  ìˆë‹¤. Time-series ì—ì„œë„ ê°€ì¥ ì í•©í•œ ëª¨ë¸ì¼ ì§€ì— ëŒ€í•´ì„œëŠ” ì¶”ê°€ì ì¸ ì—°êµ¬ê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤. (ì°¸ì¡°: [Are Transformers the most Effective model for Time Series Forecasting?](https://arxiv.org/abs/2205.13504))

**References**

- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [https://nlp.seas.harvard.edu/2018/04/03/attention.html](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
- [https://huggingface.co/blog/time-series-transformers](https://huggingface.co/blog/time-series-transformers)
- [https://lilianweng.github.io/posts/2018-06-24-attention/](https://lilianweng.github.io/posts/2018-06-24-attention/)
- [https://github.com/bentrevett/pytorch-seq2seq/blob/master/6 - Attention is All You Need.ipynb](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)
- [https://tigris-data-science.tistory.com/entry/ì°¨ê·¼ì°¨ê·¼-ì´í•´í•˜ëŠ”-Transformer5-Positional-Encoding](https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer5-Positional-Encoding)
- [https://mohitkpandey.github.io/posts/2020/11/trfm-code/](https://mohitkpandey.github.io/posts/2020/11/trfm-code/)Table of Contents

---